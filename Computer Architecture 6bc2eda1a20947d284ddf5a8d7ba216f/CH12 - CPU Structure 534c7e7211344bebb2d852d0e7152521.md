# CH12 - CPU Structure.

- Introduction.
    - Now we need to enhance our computer performance.
    - This is done by two steps :
        - Using new technology : using better hardware or circuits.
        - Enhancement organization : Using multiple registers, cache memory, and instruction pipeline.
- Instruction Pipelining.
    - Instruction can be fulfilled within multiple stages, so in a two stage instruction pipeline, you first fetch the instruction and then execute that instruction.
    - However, one advantage of pipelining instruction is that instruction execution takes longer time than fetching that instruction. So while executing an instruction, you can fetch another one meanwhile. This means that you haven’t blocked the instruction cycle just to wait an instruction to be fully fetched and executed.
    - One problem we might face is that fetching the next instruction while the previous instruction is being executed which may be infeasible in case of conditional branching because you don’t know which branch needs what instruction ahead of time.
    - Guessing might be a solution for such problem.
    - 6-Stages pipeline.
        
        ![Untitled](CH12%20-%20CPU%20Structure%20534c7e7211344bebb2d852d0e7152521/Untitled.png)
        
        - If you recall, this is the instruction cycle.
        - Consider you have 9 instructions to execute which requires 9 instruction cycles. If you didn’t use pipelining, you’d need 9 * 6 = 54 time units to implement these 9 instructions, but by using pipelining, you can reduce the time to 14 time units.
        
        ![Untitled](CH12%20-%20CPU%20Structure%20534c7e7211344bebb2d852d0e7152521/Untitled%201.png)
        
        - You should be aware that these stages are based on some assumptions :
            - Each instruction goes through all the six stages.
            - All stages can be performed in parallel except stages that need memory access cannot be performed at the same time like FI, FO, and WO.
        
        ![Untitled](CH12%20-%20CPU%20Structure%20534c7e7211344bebb2d852d0e7152521/Untitled%202.png)
        
        - Unfortunately, a problem arises due to branching. You can notice that you can’t fetch next instruction unless you wait the previous instruction to be fully finish executing and fetching.
        
        ![Untitled](CH12%20-%20CPU%20Structure%20534c7e7211344bebb2d852d0e7152521/Untitled%203.png)
        
        ![Untitled](CH12%20-%20CPU%20Structure%20534c7e7211344bebb2d852d0e7152521/Untitled%204.png)
        
        ![Untitled](CH12%20-%20CPU%20Structure%20534c7e7211344bebb2d852d0e7152521/Untitled%205.png)
        
    - Dealing with branches techniques.
        - Multiple Streams.
            - The main idea is creating more pipelines depending on the number of branches we have.
            - Each pipeline prefetches its instruction, and if it happens that you choose one pipeline, the other gets cancelled.
            - An obvious drawback is that multiple branches means multiple pipelines. Therefore bus and register contentions may occur.
        - Prefetch Branch Target
            - Say you have a single pipeline, and some instructions that needs to be executed on “true” branch, and other on “false” branch.
            - You can prefetch one instruction from “true” branch, and one instruction from “false” branch and so on until you make a decision on which branch you will continue with, therefore the other one is cancelled.
        - Loop buffer.
            - It’s a very-high speed memory that stores a set of most recent fetched instructions.
            - So if you’ve written a “while” loop, the instructions that are executed inside is revisited more often, so to make fetching these instruction faster, a buffer is used.
                
                ![Untitled](CH12%20-%20CPU%20Structure%20534c7e7211344bebb2d852d0e7152521/Untitled%206.png)
                
        - Branch Predication.
            - We have two types of prediction, static and dynamic.
                - **Static**.
                    - In static prediction, you have three pathways.
                        - **Predict never taken** : It’s assumed that jump won’t happen and you keep executing without branching.
                        - **Predict always taken** : It’s assumed that the jump will happen and you’d always fetch target instruction that is inside the branch.
                        - **Predict by opcode** : Some instructions are more likely to result in jump than others due to specific opcodes.
                    - Can lead up to 75% success.
                - **Dynamic**.
                    - In dynamic prediction, you keep track of conditional instruction execution.
                    - So for example, if you executed an instruction and predict that it will be a never taken instruction, but turns out your guess was wrong then in the next conditional instruction, you’d choose another guess than the first one.
                    - So your prediction is changing depending on the history.
                    
                    ![Untitled](CH12%20-%20CPU%20Structure%20534c7e7211344bebb2d852d0e7152521/Untitled%207.png)